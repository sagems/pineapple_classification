{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyNzpp+qnUH+lzt9rh+zSfJW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","\n","# mount drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rk2WKf1hxTe7","executionInfo":{"status":"ok","timestamp":1710753254261,"user_tz":420,"elapsed":861,"user":{"displayName":"Hannah Norman","userId":"13806459595988470382"}},"outputId":"2e7b5123-c5e9-42ef-d7ca-057883422eaa"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":["Adapted from the following tutorial: https://colab.research.google.com/github/csaybar/EEwPython/blob/master/cnn_demo.ipynb#scrollTo=cTcFXVBRQ8RN"],"metadata":{"id":"fQhrotuDu8rU"}},{"cell_type":"markdown","source":["Main steps for building the U-Net:\n","1. Create the dataset comprising 256x256 pixel patches.\n","2. Visualize data/perform some exploratory data analysis.\n","3. Set up data pipeline and complete preprocessing.\n","4. Initialize the model's parameters.\n","5. Loop:\n","  - Calculate current loss (forward propagation)\n","  - Calculate current gradient (backward propagation)\n","  - Update parameters (gradient descent)"],"metadata":{"id":"H1PR-ywuvGbN"}},{"cell_type":"markdown","source":["===================================================================================="],"metadata":{"id":"_v-Fw-ded1m-"}},{"cell_type":"markdown","source":["### Installation\n","Install required packages and extensions."],"metadata":{"id":"44l5ipsEyG24"}},{"cell_type":"code","source":["!pip install tensorflow                 # tensorflow\n","!pip install earthengine-api            # earthengine API\n","!pip install --upgrade earthengine-api\n","\n","# load the TensorBoard notebook extension\n","%load_ext tensorboard"],"metadata":{"id":"p7gAMaBQxfmh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 2. Authentication\n","Authenticate with required services."],"metadata":{"id":"n396aYvVyamm"}},{"cell_type":"code","source":["# Google Colab\n","from google.colab import auth\n","auth.authenticate_user()"],"metadata":{"id":"qIh41iu6yosG","executionInfo":{"status":"ok","timestamp":1710753258984,"user_tz":420,"elapsed":697,"user":{"displayName":"Hannah Norman","userId":"13806459595988470382"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# Google Earth Engine\n","!earthengine authenticate --force"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JgFjlBOMyx4F","executionInfo":{"status":"ok","timestamp":1710753282011,"user_tz":420,"elapsed":5619,"user":{"displayName":"Hannah Norman","userId":"13806459595988470382"}},"outputId":"d64a66e6-e3e5-480c-ef05-a2ae778a7a4f"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["W0318 09:14:40.472917 132048745291776 _default.py:683] No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n"]}]},{"cell_type":"markdown","source":["Initialize and test software set-up."],"metadata":{"id":"dKPfXEvu1hMT"}},{"cell_type":"code","source":["# Earth Engine Python API\n","import ee\n","print('Earth Engine version: ' + ee.__version__)\n","# ee.Initialize(project='gbsc-gcp-lab-emordeca')\n","\n","# Tensorflow\n","import tensorflow as tf\n","print('Tensorflow version: ' + tf.__version__)\n","\n","# Folium\n","import folium\n","print('Folium version: ' + folium.__version__)\n","\n","# define the URL format used for Earth Engine generated map tiles\n","EE_TILES = 'https://earthengine.googleapis.com/map/{mapid}/{{z}}/{{x}}/{{y}}?token={token}'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZAW67Wok1aCq","executionInfo":{"status":"ok","timestamp":1710753411907,"user_tz":420,"elapsed":4883,"user":{"displayName":"Hannah Norman","userId":"13806459595988470382"}},"outputId":"adecd648-3c91-48db-da54-89cb77976c6a"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Earth Engine version: 0.1.394\n","Tensorflow version: 2.15.0\n","Folium version: 0.14.0\n"]}]},{"cell_type":"markdown","source":["### Prepare Dataset\n","\n","First, we want to define our prediction area (Costa Rica) and pass it to GEE. In order to move a vector to GEE, we'll use the `ee.Geometry.*` module. We'll also define a `map_display()` function for displaying a map on screen."],"metadata":{"id":"Zx775z_v2gzb"}},{"cell_type":"code","execution_count":7,"metadata":{"id":"k_aIAVPnu5BA","executionInfo":{"status":"ok","timestamp":1710753608005,"user_tz":420,"elapsed":136,"user":{"displayName":"Hannah Norman","userId":"13806459595988470382"}}},"outputs":[],"source":["def map_display(center, eeg_dict, tiles=\"OpensTreetMap\", zoom=10):\n","    '''\n","    param center: Center of the map (Latitude and Longitude).\n","    param eeg_dict: Earth Engine Geometries or Tiles dictionary.\n","    param tiles: Mapbox Bright, Mapbox Control Room, Stamen Terrain, Stamen Toner, stamenwatercolor, cartodbpositron.\n","    param zoom: Initial zoom level for the map.\n","\n","    return: A folium.Map object.\n","    '''\n","    mapViz = folium.Map(location=center, tiles=tiles, zoom_start=zoom)\n","    for k,v in eeg_dict.items():\n","      if ee.image.Image in [type(x) for x in v.values()]:\n","        folium.TileLayer(\n","            tiles = EE_TILES.format(**v),\n","            attr  = 'Google Earth Engine',\n","            overlay =True,\n","            name  = k\n","          ).add_to(mapViz)\n","      else:\n","        folium.GeoJson(\n","        data = v,\n","        name = k\n","          ).add_to(mapViz)\n","    mapViz.add_child(folium.LayerControl())\n","    return mapViz"]},{"cell_type":"code","source":["# prediction area\n","# TODO: update these coords\n","xmin, ymin, xmax, ymax = [-72.778645, -16.651663, -72.66865, -16.57553]\n","\n","# passing the prediction area to Earth Engine\n","costa_rica = ee.Geometry.Rectangle([xmin,ymin,xmax,ymax])\n","center = costa_rica.centroid().getInfo()['coordinates']\n","center.reverse()\n","map_display(center, {'Costa Rica':costa_rica.getInfo()}, zoom=12)"],"metadata":{"id":"JKVg2C7NxPuY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Next, we'll read in the train/test dataset and create a visualization.\n","\n","- train dataset (2800 points):\n","\n","  - 1400 labeled as \"pineapple\"\n","  - 1400 labeled as \"non-pineapple\"\n","\n","- test dataset (700 points):\n","\n","  - 350 labeled as \"pineapple\"\n","  - 350 labeled as \"non-pineapple\""],"metadata":{"id":"M5n_O4f-3aLy"}},{"cell_type":"code","source":["# import the train/test dataset\n","# TODO: update these filepaths, and divvy into train/test sets\n","train_pineapple = ee.FeatureCollection('users/csaybar/DLdemos/train_set')\n","test_pineapple = ee.FeatureCollection('users/csaybar/DLdemos/test_set')\n","\n","\"\"\"\n","background_2018 = ee.FeatureCollection('users/sagems/background_2018')\n","pineapple_2018 = ee.FeatureCollection('users/sagems/pineapple_2018')\n","background_2019 = ee.FeatureCollection('users/sagems/background_2019')\n","pineapple_2019 = ee.FeatureCollection('users/sagems/pineapple_2019')\n","borders = ee.FeatureCollection('FAO/GAUL/2015/level0')\n","costa_rica = borders.filter(ee.Filter.eq('ADM0_NAME', 'Costa Rica'))\n","\"\"\"\n","\n","# display the train/test dataset\n","db_crop = train_pineapple.merge(test_pineapple)\n","center = db_crop.geometry().centroid().getInfo()['coordinates']\n","center.reverse()\n","\n","eeg_dict = {'train': train_pineapple.draw(**{'color': 'FF0000', 'strokeWidth': 5}).getMapId(),\n","            'test' : test_pineapple.draw(**{'color': '0000FF', 'strokeWidth': 5}).getMapId(),\n","            'CostaRica': costa_rica.getInfo()\n","            }\n","\n","map_display(center, eeg_dict, zoom=8)"],"metadata":{"id":"jD1dOY8r3_4k"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["For training the model, we will use the Harmonized Sentinel 2 dataset with images rendered at a 30-meter spatial resolution."],"metadata":{"id":"W8lTWBIJ5kn5"}},{"cell_type":"code","source":["from collections import OrderedDict\n","\n","# load the dataset\n","# TODO: update these filepaths\n","s2 = ee.ImageCollection('users/csaybar/GLC30PERU').max().eq(10).rename('target')\n","\n","# vizualize the dataset\n","s2_id = s2.getMapId()\n","eeg_dict['Sentinel2'] = s2_id\n","\n","# change the order of the dictionary\n","key_order = ['Sentinel2', 'CostaRica', 'train', 'test']\n","eeg_dict = OrderedDict((k, eeg_dict[k]) for k in key_order)\n","\n","map_display(center, eeg_dict, zoom=8)"],"metadata":{"id":"5U3PXuAs5Gep"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, we're going to obtain the input data for mapping pineapple plantation areas from the Copernicus sensor (???). Also, we're going to define a function to mask cloud-cover in pixels.\n","\n","The cloud-masking function from the tutorial is included as `maskS2clouds_tutorial()`."],"metadata":{"id":"NNuwjWaS6bDk"}},{"cell_type":"code","source":["def maskS2clouds(image):\n","    '''\n","    param image: Image input.\n","\n","    return: Cloud-masked image.\n","    '''\n","    qa = image.select('QA10')\n","\n","    # bits 10 and 11 are clouds and cirrus, respectively\n","    cloud_bit_mask = 1 << 10\n","    cirrus_bit_mask = 1 << 11\n","\n","    # both flags should be set to zero, indicating clear conditions\n","    mask = qa.bitwiseAnd(cloud_bit_mask).eq(0).And(qa.bitwiseAnd(cirrus_bit_mask).eq(0))\n","    return image.updateMask(mask).divide(10000)"],"metadata":{"id":"fOwd0V3v7Xow","executionInfo":{"status":"ok","timestamp":1710754884362,"user_tz":420,"elapsed":3,"user":{"displayName":"Hannah Norman","userId":"13806459595988470382"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["def maskS2clouds_tutorial(image):\n","  '''\n","  Function to mask clouds based on the pixel_qa band of Landsat 5 data. See:\n","  https://developers.google.com/earth-engine/datasets/catalog/LANDSAT_LT05_C01_T1_SR\n","\n","  param img: image input Landsat 5 SR image\n","\n","  return: cloud-masked Landsat 5 image\n","  '''\n","  qa = image.select('pixel_qa')\n","  cloud = qa.bitwiseAnd(1 << 5)\\\n","            .And(qa.bitwiseAnd(1 << 7))\\\n","            .Or(qa.bitwiseAnd(1 << 3))\n","  mask2 = image.mask().reduce(ee.Reducer.min())\n","  return image.updateMask(cloud.Not()).updateMask(mask2)\n"],"metadata":{"id":"gjOSoBs97Jlx","executionInfo":{"status":"ok","timestamp":1710754885541,"user_tz":420,"elapsed":251,"user":{"displayName":"Hannah Norman","userId":"13806459595988470382"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["Now we will filter and reduce the entire Sentinel-2 dataset, considering the following:\n","\n","- Select only bands [R, G, B, NIR].\n","\n","- Filter by cloud pixel coverage by scene (< 20%).\n","\n","- Filter by date (the tutorial selects 1 year)\n","\n","- Apply `mask2cloud()` function to each image.\n","\n","- Get the median of the ImageCollection.\n","\n","- Clip the image considering the study area."],"metadata":{"id":"IR5ZnEZE8Cj5"}},{"cell_type":"code","source":["# prepare the satellite image (Sentinel-2)\n","RGB_bands = ['B3','B2','B1']  # RGB\n","NDVI_bands = ['B4','B3']      # NIR\n","\n","# NOTE: tutorial has 2 different satellite sensor source things, but I'm under the impression we're\n","#       using Sentinel-2 for both, just at different times? idk what's going on here lol\n","# TODO: change this to our stuff\n","s2_cop = ee.ImageCollection(\"LANDSAT/LT05/C01/T1_SR\")\\\n","               .filterBounds(db_crop)\\\n","               .filterDate('2005-01-01', '2006-12-31')\\\n","               .filter(ee.Filter.lt('CLOUD_COVER', 20))\\\n","               .map(maskS2clouds)\\\n","               .median()\\\n","               .multiply(0.0001)\n","\n","s2_cop_ndvi = s2_cop.normalizedDifference(NDVI_bands).rename(['NDVI'])\n","s2_cop_rgb = s2_cop.select(RGB_bands).rename(['R','G','B'])\n","s2_cop = s2_cop_rgb.addBands(s2_cop_ndvi).addBands(s2_cop)"],"metadata":{"id":"zbHRlpQh7VGU","executionInfo":{"status":"aborted","timestamp":1710754714524,"user_tz":420,"elapsed":4,"user":{"displayName":"Hannah Norman","userId":"13806459595988470382"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from collections import OrderedDict\n","\n","# create a visualization with Folium\n","visParams_s2_cop = {\n","  'bands': ['R', 'G', 'B'],\n","  'min': 0,\n","  'max': 0.5,\n","  'gamma': 1.4,\n","}\n","\n","s2_cop_mapID = s2.getMapId(visParams_s2_cop)\n","eeg_dict['Sentinel2_Copernicus'] = s2_cop_mapID\n","\n","# change the order of the dictionary\n","key_order = ['Sentinel2_Copernicus', 'Sentinel2', 'CostaRica', 'train', 'test']\n","eeg_dict = OrderedDict((k, eeg_dict[k]) for k in key_order)\n","\n","map_display(center, eeg_dict, zoom=8)"],"metadata":{"id":"YkllvqBi9Uw4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The key to success for integrating GEE with a CNN pipeline is the `ee.Image.neighborhoodToArray()` function. It turns the neighborhood of each pixel in a scalar image into a 2D array (see image below). Axes 0 and 1 of the output array correspond to Y and X axes of the image, respectively. The output image will have as many bands as the input; each output band has the same mask as the corresponding input band. The footprint and metadata of the input image are preserved.\n","\n","There are several restrictions about the max number of features that a table can save (~ 10 M). For this reason, the `saveCNN_batch()` function is created as below:"],"metadata":{"id":"V_vJNTpGAB7C"}},{"cell_type":"code","source":["import numpy as np\n","import time\n","\n","def saveCNN_batch(image, point, kernel_size, scale, file_prefix, selectors, folder, bucket='bag_csaybar'):\n","  \"\"\"\n","    Export a dataset for semantic segmentation by batches\n","\n","  Params:\n","  ------\n","    - image : ee.Image to get pixels from; must be scalar-valued.\n","    - point : Points to sample over.\n","    - kernel_size : The kernel specifying the shape of the neighborhood. Only fixed, square and rectangle kernels are supported.\n","      Weights are ignored; only the shape of the kernel is used.\n","    - scale : A nominal scale in meters of the projection to work in.\n","    - file_prefix : Cloud Storage object name prefix for the export.\n","    - selector : Specified the properties to save.\n","    - bucket : The name of a Cloud Storage bucket for the export.\n","  \"\"\"\n","  print('Found Cloud Storage bucket.' if tf.io.gfile.exists('gs://' + bucket)\n","    else 'Output Cloud Storage bucket does not exist.')\n","\n","  # download the points (Server -> Client)\n","  nbands = len(selectors)\n","  points = train_pineapple.geometry().getInfo()['coordinates']\n","  nfeatures = kernel_size*kernel_size*nbands*len(points) #estimate the totals # of features\n","\n","  image_neighborhood = image.neighborhoodToArray(ee.Kernel.rectangle(kernel_size, kernel_size, 'pixels'))\n","  filenames = []\n","\n","  # threshold considering the max number of features permitted to export\n","  if nfeatures > 3e6:\n","    nparts = int(np.ceil(nfeatures/3e6))\n","    print('Dataset too long, splitting it into '+ str(nparts),'equal parts.')\n","\n","    nppoints = np.array(points)\n","    np.random.shuffle(nppoints)\n","\n","    count_batch = 1\n","\n","    for batch_arr in np.array_split(nppoints,nparts):\n","\n","      fcp = ee.FeatureCollection([\n","          ee.Feature(ee.Geometry.Point(p),{'class':'NA'})\n","          for p in batch_arr.tolist()\n","      ])\n","\n","      # pineapple dataset (fcp-points) collocation to each S2 grid cell value\n","      train_db = image_neighborhood.sampleRegions(collection=fcp, scale=scale)\n","      filename = '%s/%s-%04d_' % (folder,file_prefix,count_batch)\n","\n","      # create the tasks for passing of GEE to Google storage\n","      print('sending the task #%04d'%count_batch)\n","      Task = ee.batch.Export.table.toCloudStorage(\n","        collection=train_db,\n","        selectors=selectors,\n","        description='Export batch '+str(count_batch),\n","        fileNamePrefix=filename,\n","        bucket=bucket,\n","        fileFormat='TFRecord')\n","\n","      Task.start()\n","      filenames.append(filename)\n","      count_batch+=1\n","\n","      while Task.active():\n","        print('Polling for task (id: {}).'.format(Task.id))\n","        time.sleep(3)\n","\n","    return filenames\n","\n","  else:\n","    train_db = image_neighborhood.sampleRegions(collection=points, scale=scale)\n","    Task = ee.batch.Export.table.toCloudStorage(\n","      collection=train_db,\n","      selectors=selectors,\n","      description='Training Export',\n","      fileNamePrefix=file_prefix,\n","      bucket=bucket,\n","      fileFormat='TFRecord')\n","    Task.start()\n","\n","    while Task.active():\n","      print('Polling for task (id: {}).'.format(Task.id))\n","      time.sleep(3)\n","\n","    return file_prefix"],"metadata":{"id":"-Dzfw7QcAI36"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Unfortunately, you cannot use Tensorflow directly in Earth Engine. To overcome this limitation, the function `saveCNN_batch()` use Google Cloud Storage Bucket (you could use Google Drive instead) to save the dataset since both GEE and Tensorflow can access to it."],"metadata":{"id":"xDBW_hERAs0B"}},{"cell_type":"code","source":["selectors = ['R','G','B','NDVI','target']\n","\n","# TODO: I think the folder and bucket params, at the very least, need to be changed\n","train_filenames = saveCNN_batch(s2_cop, train_pineapple, 128, 30, 'trainUNET', selectors, folder='unet', bucket='csaybar')\n","test_filenames = saveCNN_batch(s2_cop, test_pineapple, 128, 30, 'testUNET', selectors, folder='unet', bucket='csaybar')"],"metadata":{"id":"rdplP4ooA0SQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Create a TensorFlow Dataset\n","\n","Read data from the TFRecord file into a tf.data.Dataset. Pre-process the dataset to get it into a suitable format for input into the DNN model."],"metadata":{"id":"dpmf2OHrBTdh"}},{"cell_type":"code","source":["def input_fn(fileNames, num_epochs=None, shuffle=True, batch_size=16, side = 257):\n","  # read `TFRecordDatasets`\n","  dataset = tf.data.TFRecordDataset(fileNames, compression_type='GZIP')\n","\n","  # names of the features\n","  feature_columns = {\n","    'R': tf.io.FixedLenFeature([side, side], dtype=tf.float32),\n","    'G': tf.io.FixedLenFeature([side, side], dtype=tf.float32),\n","    'B': tf.io.FixedLenFeature([side, side], dtype=tf.float32),\n","    'NDVI': tf.io.FixedLenFeature([side, side], dtype=tf.float32),\n","    'target': tf.io.FixedLenFeature([side, side], dtype=tf.float32)\n","  }\n","\n","  # parsing function\n","  def parse(example_proto):\n","    parsed_features = tf.io.parse_single_example(example_proto, feature_columns)\n","    parsed_features = {key:value[1:side,1:side] for key,value in parsed_features.items()}\n","\n","    # separate the class labels from the training features\n","    labels = parsed_features.pop('target')\n","    return parsed_features, tf.cast(labels, tf.int32)\n","\n","  # convert FeatureColumns to a 4D tensor\n","  def stack_images(features,label):\n","    nfeat = tf.transpose(tf.squeeze(tf.stack(list(features.values()))))\n","    nlabel = (tf.transpose(label))[:,:,tf.newaxis]\n","    return nfeat, nlabel\n","\n","  dataset = dataset.map(parse, num_parallel_calls=4)\n","  dataset = dataset.map(stack_images, num_parallel_calls=4)\n","\n","  if shuffle:\n","    dataset = dataset.shuffle(buffer_size = batch_size * 10)\n","\n","  dataset = dataset.batch(batch_size)\n","  dataset = dataset.repeat(num_epochs)\n","\n","  return dataset"],"metadata":{"id":"PAD0ErDvQ8fU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# TODO: update these filepaths\n","folder = 'unet'\n","bucket = 'bag_csaybar'\n","\n","files_list = !gsutil ls 'gs://'{bucket}'/'{folder}\n","\n","train_file_prefix = 'trainUNET'\n","train_file_path = [s for s in files_list if train_file_prefix in s]\n","\n","test_file_prefix = 'testUNET'\n","test_file_path = [s for s in files_list if test_file_prefix in s]"],"metadata":{"id":"GE5H59-VA-21"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dba = input_fn(train_file_path, num_epochs=100, shuffle=True, batch_size=3)\n","test_dba = input_fn(test_file_path, num_epochs=1, shuffle=False, batch_size=1)"],"metadata":{"id":"YRJAi8tBRbhz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Visualize Patches\n","Visualize some of the map patches in the dataset."],"metadata":{"id":"Vyu5XeYtRxGs"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import numpy as np"],"metadata":{"id":"FWLygtpGR-0j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["display_num = 5\n","plt.figure(figsize=(14, 21))\n","\n","c = 0\n","for i in range(1, display_num):\n","  for x in test_dba.take(i):\n","    x\n","\n","  tensor = tf.squeeze(x[0]).numpy()[:,:,[3,1,0]]\n","  target = tf.squeeze(x[1])\n","\n","  # print(target.sum())\n","  plt.subplot(display_num, 2, c + 1)\n","  plt.imshow(tensor)\n","  plt.title(\"RGB SENTINEL-2\")\n","\n","  plt.subplot(display_num, 2, c + 2)\n","  plt.imshow(target)\n","  plt.title(\"Pineapple Plantations\")\n","  c += 2\n","\n","plt.show()"],"metadata":{"id":"JiaW_wPTRynQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Set-Up Params\n","We'll begin by defining some constant parameters."],"metadata":{"id":"-yrOL9QySOlQ"}},{"cell_type":"code","source":["IMG_SHAPE = (256, 256, 4)\n","EPOCHS = 10"],"metadata":{"id":"IgXc0IqdSSlE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Create U-Net Model\n","\n","Here, we'll create a convolutional neural network model with:\n","- 5 encoder layers.\n","- 5 decoder layer.\n","- 1 output layer.\n","\n","The **encoder layer** is composed of a linear stack of Conv, BatchNorm, and Relu operations followed by a MaxPool. Each MaxPool will reduce the spatial resolution of our feature map by a factor of 2. We keep track of the outputs of each block as we feed these high-resolution feature maps with the decoder portion.\n","\n","The **decoder layer** is comprised of UpSampling2D, Conv, BatchNorm, and Relu. Note that we concatenate the feature map of the same size on the decoder side.\n","\n","Finally, for the **output layer**, we add a Conv operation that performs a convolution along the channels for each individual pixel (kernel size of (1, 1)) that outputs our final segmentation mask in grayscale.\n","\n","Additionally, early stopping, TensorBoard, and best-model callback are utilized."],"metadata":{"id":"KlpoZbiiSVh8"}},{"cell_type":"code","source":["from tensorflow.keras import layers\n","\n","def conv_block(input_tensor, num_filters):\n","  encoder = layers.Conv2D(num_filters, (3, 3), padding='same')(input_tensor)\n","  encoder = layers.BatchNormalization()(encoder)\n","  encoder = layers.Activation('relu')(encoder)\n","  encoder = layers.Conv2D(num_filters, (3, 3), padding='same')(encoder)\n","  encoder = layers.BatchNormalization()(encoder)\n","  encoder = layers.Activation('relu')(encoder)\n","  return encoder\n","\n","def encoder_block(input_tensor, num_filters):\n","  encoder = conv_block(input_tensor, num_filters)\n","  encoder_pool = layers.MaxPooling2D((2, 2), strides=(2, 2))(encoder)\n","\n","  return encoder_pool, encoder\n","\n","def decoder_block(input_tensor, concat_tensor, num_filters):\n","  decoder = layers.Conv2DTranspose(num_filters, (2, 2), strides=(2, 2), padding='same')(input_tensor)\n","  decoder = layers.concatenate([concat_tensor, decoder], axis=-1)\n","  decoder = layers.BatchNormalization()(decoder)\n","  decoder = layers.Activation('relu')(decoder)\n","  decoder = layers.Conv2D(num_filters, (3, 3), padding='same')(decoder)\n","  decoder = layers.BatchNormalization()(decoder)\n","  decoder = layers.Activation('relu')(decoder)\n","  decoder = layers.Conv2D(num_filters, (3, 3), padding='same')(decoder)\n","  decoder = layers.BatchNormalization()(decoder)\n","  decoder = layers.Activation('relu')(decoder)\n","  return decoder\n","\n","inputs = layers.Input(shape=IMG_SHAPE)\n","# 256\n","encoder0_pool, encoder0 = encoder_block(inputs, 32)\n","# 128\n","encoder1_pool, encoder1 = encoder_block(encoder0_pool, 64)\n","# 64\n","encoder2_pool, encoder2 = encoder_block(encoder1_pool, 128)\n","# 32\n","encoder3_pool, encoder3 = encoder_block(encoder2_pool, 256)\n","# 16\n","encoder4_pool, encoder4 = encoder_block(encoder3_pool, 512)\n","# 8\n","center = conv_block(encoder4_pool, 1024)\n","# center\n","decoder4 = decoder_block(center, encoder4, 512)\n","# 16\n","decoder3 = decoder_block(decoder4, encoder3, 256)\n","# 32\n","decoder2 = decoder_block(decoder3, encoder2, 128)\n","# 64\n","decoder1 = decoder_block(decoder2, encoder1, 64)\n","# 128\n","decoder0 = decoder_block(decoder1, encoder0, 32)\n","# 256\n","\n","outputs = layers.Conv2D(1, (1, 1), activation='sigmoid')(decoder0)"],"metadata":{"id":"ZA88Y3KyXjVe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Define the Model\n","\n","We'll define the model by specifying the associated inputs and outputs."],"metadata":{"id":"UvDtq9B8XwxR"}},{"cell_type":"code","source":["from tensorflow.keras import models\n","model = models.Model(inputs=[inputs], outputs=[outputs])\n","model.summary()"],"metadata":{"id":"00bNp2rpX5s0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Define Loss Function\n","\n","Defining loss and metric functions are simple with Keras. Just define a function that takes both the True and Predicted labels for a given example. Dice loss is a metric that measures overlap. We use dice loss here because it performs better with class imbalanced problems by design. Using cross entropy (another loss function) is more of a proxy which is easier to maximize. Instead, we maximize our objective directly."],"metadata":{"id":"9Pp37tAoX-vS"}},{"cell_type":"code","source":["from tensorflow.keras import losses\n","\n","def dice_coeff(y_true, y_pred):\n","    smooth = 1.\n","    # flatten\n","    y_true_f = tf.reshape(y_true, [-1])\n","    y_pred_f = tf.reshape(y_pred, [-1])\n","    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n","    score = (2. * intersection + smooth) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + smooth)\n","    return score\n","\n","def dice_loss(y_true, y_pred):\n","    loss = 1 - dice_coeff(y_true, y_pred)\n","    return loss\n","\n","def bce_dice_loss(y_true, y_pred):\n","  loss = losses.binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n","  return loss"],"metadata":{"id":"rEeJ2wwpYAxo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Compile the Model\n","\n","We'll use our custom loss function to minimize. In addition, we specify what metrics we want to keep track of as we train. Note that metrics are not actually used during the training process to tune the parameters, but are instead used to measure performance of the training process."],"metadata":{"id":"syOH5_6gYbs9"}},{"cell_type":"code","source":["model.compile(optimizer='adam', loss=bce_dice_loss, metrics=[dice_loss])"],"metadata":{"id":"9K8KjrB2YlDl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Train the Model\n","\n","Training your model with tf.data involves simply providing the model's fit function with your training/validation dataset, the number of steps, and epochs.\n","\n","We also include a Model callback, `ModelCheckpoint`, that will save the model to disk after each epoch. We configure it so that it only saves our highest performing model. Note that saving the model captures more than just the weights of the model: by default, it saves the model architecture, weights, as well as information about the training process such as the state of the optimizer, etc."],"metadata":{"id":"MFflmcNaY2S1"}},{"cell_type":"code","source":["from tensorflow import keras\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","import os\n","import datetime\n","\n","# callbacks time\n","logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n","tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n","\n","# early stopping\n","es = EarlyStopping(monitor='val_loss', patience=10)\n","\n","# model checkpoint\n","mcp = ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)"],"metadata":{"id":"zsrz2KDUY3Ty"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["n_train = 550\n","batch_size = 3\n","\n","# train the model (tutorial stops after 15min train time)\n","history = model.fit(train_dba,\n","                    steps_per_epoch=int(np.ceil(n_train / float(batch_size))),\n","                    epochs=EPOCHS,\n","                    validation_data=test_dba,\n","                    callbacks=[tensorboard_callback,es,mcp])"],"metadata":{"id":"DLnvHBRVZLYy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%tensorboard --logdir logs\n","#!kill 607"],"metadata":{"id":"aCr6YMrqZSJk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Evaluate the Model"],"metadata":{"id":"4aq6NTq3Zluc"}},{"cell_type":"code","source":["model.evaluate(x=test_dba)"],"metadata":{"id":"IPSX6XyJZnCQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Prediction"],"metadata":{"id":"sUBBRNVOZp2V"}},{"cell_type":"code","source":["# TODO: update these filepaths with 2020-23 data\n","s2_cop = ee.ImageCollection(\"LANDSAT/LT05/C01/T1_SR\")\\\n","               .filterBounds(costa_rica)\\\n","               .filterDate('2005-01-01', '2006-12-31')\\\n","               .filter(ee.Filter.lt('CLOUD_COVER', 20))\\\n","               .map(maskS2clouds)\\\n","               .median()\\\n","               .multiply(0.0001)\n","\n","s2_cop_ndvi = s2_cop.normalizedDifference(NDVI_bands).rename(['NDVI'])\n","s2_cop_rgb = s2_cop.select(RGB_bands).rename(['R','G','B'])\n","s2_cop = s2_cop_rgb.addBands(s2_cop_ndvi)"],"metadata":{"id":"TIlxHdePZtYW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from collections import OrderedDict\n","\n","# vizualize the dataset\n","s2_cop_id = s2_cop.clip(costa_rica.buffer(2500)).getMapId({'max':0.6,'min':0})\n","center = costa_rica.centroid().getInfo()['coordinates']\n","center.reverse()\n","map_display(center, {'s2_cop_id':l5id}, zoom=11)"],"metadata":{"id":"ZpNknLcHaD26"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To export the results to Google Cloud Storage, it's best to define the following format options parameters in order to save memory:\n","\n","- **patchDimensions**: Patch dimensions tiled over the export area, covering every pixel in the bounding box exactly once (except when the patch dimensions do not evenly divide the bounding box in which case the lower and right sides are trimmed).\n","\n","- **compressed**: If true, compresses the .tfrecord files with gzip and appends the \".gz\" suffix."],"metadata":{"id":"QoKVtPLFaZVk"}},{"cell_type":"code","source":["# TODO: update these filepaths\n","output_bucket = 'bag_csaybar'\n","image_file_prefix = 'unet/Predict_CamanaValleyCrop'\n","\n","# specify file dimensions\n","image_export_format_options = {\n","  'patchDimensions': [256, 256],\n","  'compressed': True\n","}\n","\n","# set-up the task\n","image_task = ee.batch.Export.image.toCloudStorage(\n","  image=l5,\n","  description='Image Export',\n","  fileNamePrefix=image_file_prefix,\n","  bucket=output_bucket,\n","  scale=30,\n","  fileFormat='TFRecord',\n","  region=Camana_valley.buffer(2500).getInfo()['coordinates'],\n","  formatOptions=image_export_format_options,\n",")\n","\n","# start the export\n","image_task.start()"],"metadata":{"id":"v7zLJL3TarGT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import time\n","while image_task.active():\n","  print('Polling for task (id: {}).'.format(imageTask.id))\n","  time.sleep(5)"],"metadata":{"id":"xayltLg6a2-4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now it's time to classify the image that was exported from GEE to GCS using Tensorflow. If the exported image is large, it will be split into multiple TFRecord files in its destination folder. There will also be a JSON sidecar file called \"the mixer\" that describes the format and georeferencing of the image. Here, we will find the image files and the mixer file, getting some info out of the mixer that will be useful during model inference."],"metadata":{"id":"0wZnH4s1a56v"}},{"cell_type":"code","source":["files_list = !gsutil ls 'gs://'{output_bucket}'/unet/'\n","export_files_list = [s for s in files_list if image_file_prefix in s]\n","\n","# get the list of image files and JSON mixer file\n","image_files_list = []\n","json_file = None\n","for f in export_files_list:\n","  if f.endswith('.tfrecord.gz'):\n","    image_files_list.append(f)\n","  elif f.endswith('.json'):\n","    json_file = f\n","\n","# ensure files are in correct order\n","print(json_file)"],"metadata":{"id":"ubKt5m1DbE-U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The mixer contains metadata and georeferencing information for the exported patches, each of which is in a different file. Read the mixer to get some information needed for prediction."],"metadata":{"id":"umdUDZBAbagw"}},{"cell_type":"code","source":["import json\n","from pprint import pprint\n","\n","# load the contents of the mixer file to a JSON object\n","jsonText = !gsutil cat {jsonFile}\n","\n","# get a single string w/ newlines from the IPython.utils.text.SList\n","mixer = json.loads(jsonText.nlstr)\n","pprint(mixer)"],"metadata":{"id":"Flcv2eDNbeK8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The next function is slightly different from the to the previously defined `input_fn()`. Mainly, this is because the pixels are written into records as patches, and we need to read the patches in as one big tensor (one patch for each band), and then flatten them into lots of little tensors. Once the `predict_input_fn()` is defined, it can handle the shape of the image data, so all you need to do is feed it directly to the trained model to make predictions."],"metadata":{"id":"LDTwbZmPbjKX"}},{"cell_type":"code","source":["def predict_input_fn(file_names, side, bands):\n","\n","  # read `TFRecordDatasets`\n","  dataset = tf.data.TFRecordDataset(file_names, compression_type='GZIP')\n","\n","  features_dict = {x:tf.io.FixedLenFeature([side, side], dtype=tf.float32) for x in bands}\n","\n","  # parsing function\n","  def parse_image(example_proto):\n","    parsed_features = tf.io.parse_single_example(example_proto, features_dict)\n","    return parsed_features\n","\n","  def stack_images(features):\n","    nfeat = tf.transpose(tf.squeeze(tf.stack(list(features.values()))))\n","    return nfeat\n","\n","  dataset = dataset.map(parse_image, num_parallel_calls=4)\n","  dataset = dataset.map(stack_images, num_parallel_calls=4)\n","  dataset = dataset.batch(side*side)\n","\n","  return dataset"],"metadata":{"id":"WmyeutvUb1XX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["predict_db = predict_input_fn(fileNames=image_files_list, side=256, bands=['R', 'G', 'B', 'NDVI'])\n","predictions = model.predict(predict_db)"],"metadata":{"id":"diHci6-KcBGj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now that there's a `np.array` of probabilities in \"predictions\", it's time to write them back into a file. You will write directly from TensorFlow to a file in the output Cloud Storage bucket.\n","\n","Iterate over the list and write the probabilities in patches. Specifically, we need to write the pixels into the file as patches in the same order they came out. The records are written as serialized `tf.train.Example` protos. This might take a while."],"metadata":{"id":"QWwWEY7PcG4Z"}},{"cell_type":"code","source":["# instantiate the writer\n","# TODO: change filepaths\n","PATCH_WIDTH , PATCH_HEIGHT = [256,256]\n","output_image_file = 'gs://' + output_bucket + '/unet/CamanaValleyCrop.TFRecord'\n","writer = tf.io.TFRecordWriter(output_image_file)\n","\n","# Every patch-worth of predictions we'll dump an example into the output\n","# file with a single feature that holds our predictions. Since our predictions\n","# are already in the order of the exported data, the patches we create here\n","# will also be in the right order.\n","\n","curPatch = 1\n","for  prediction in predictions:\n","  patch = prediction.squeeze().T.flatten().tolist()\n","\n","  if (len(patch) == PATCH_WIDTH * PATCH_HEIGHT):\n","    print('Done with patch ' + str(curPatch) + '...')\n","    # Create an example\n","    example = tf.train.Example(\n","      features=tf.train.Features(\n","        feature={\n","          'crop_prob': tf.train.Feature(\n","              float_list=tf.train.FloatList(\n","                  value=patch))\n","        }\n","      )\n","    )\n","\n","    writer.write(example.SerializeToString())\n","    curPatch += 1\n","\n","writer.close()"],"metadata":{"id":"YpwyLz1_cRU0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Upload Predictions to EEG\n","\n","At this stage, there should be a predictions TFRecord file sitting in the output Cloud Storage bucket. Use the gsutil command to verify that the predictions image (and associated mixer JSON) exist and have non-zero size."],"metadata":{"id":"XXWnBiE1csE0"}},{"cell_type":"code","source":["!gsutil ls -l {output_image_file}"],"metadata":{"id":"8mf3-j7EczVq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","Upload the image to Earth Engine directly from the Cloud Storage bucket with the earthengine command. Provide both the image TFRecord file and the JSON file as arguments to earthengine upload."],"metadata":{"id":"H0OD7AAuc5_R"}},{"cell_type":"code","source":["# TODO: update username\n","# TODO: update filepaths\n","USERNAME = 'csaybar'\n","output_asset_ID = 'users/' + USERNAME + '/CamanaCrop_UNET'\n","print('Writing to ' + output_asset_ID)"],"metadata":{"id":"Hz4rllc9c6n3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# start the upload (it might take a while)\n","!earthengine upload image --asset_id={output_asset_ID} {output_image_file} {json_file}"],"metadata":{"id":"800zWBykdF0q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Display Results\n","\n","Display the results using Folium!"],"metadata":{"id":"UmWmsFosdRDq"}},{"cell_type":"code","source":["probs_image = ee.Image(output_asset_ID)\n","predictions_image = ee.Image(output_asset_ID).gte(0.500)\n","eeg_dict = {'PineappleProbability':probs_image.getMapId({'min':0.49,'max':0.498}),\n","            'Pineapple':predictions_image.getMapId()}\n","\n","center = costa_rica.centroid().getInfo()['coordinates']\n","center.reverse()\n","\n","map_display(center, eeg_dict, zoom=13)"],"metadata":{"id":"_S1GOAQNdT1I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["All done!!!"],"metadata":{"id":"vpIpHhQEdwzN"}},{"cell_type":"code","source":[],"metadata":{"id":"wAsJv0xvdxXE"},"execution_count":null,"outputs":[]}]}